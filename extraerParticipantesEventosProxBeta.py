#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FLOWAGILITY SCRAPER COMPLETO - SISTEMA DE EXTRACCIÃ“N DE DATOS DE COMPETICIONES

DESCRIPCIÃ“N DEL PROCESO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Este sistema automatizado realiza un proceso completo de extracciÃ³n, transformaciÃ³n
y carga (ETL) de datos desde la plataforma FlowAgility.com. El proceso consta de
4 etapas principales:

1. EXTRACCIÃ“N DE EVENTOS BÃSICOS:
   - Login automÃ¡tico en FlowAgility
   - NavegaciÃ³n a la pÃ¡gina de eventos
   - Scroll completo para cargar todos los eventos
   - ExtracciÃ³n de campos esenciales: nombre, fechas, organizaciÃ³n, club, lugar,
     enlaces (info y participantes), y bandera del paÃ­s

2. EXTRACCIÃ“N DE INFORMACIÃ“N DETALLADA:
   - Visita a cada pÃ¡gina de informaciÃ³n de evento
   - PreservaciÃ³n de campos originales
   - Enriquecimiento con informaciÃ³n adicional
   - Mejora de datos de club y lugar si es necesario

3. EXTRACCIÃ“N DE PARTICIPANTES:
   - Acceso a pÃ¡ginas de listas de participantes
   - ExtracciÃ³n de informaciÃ³n de competidores
   - Almacenamiento individual por evento
   - ConsolidaciÃ³n en archivo Ãºnico

4. GENERACIÃ“N DE OUTPUT FINAL:
   - CreaciÃ³n de archivo JSON unificado
   - GeneraciÃ³n de CSV procesado
   - Metadata completa del proceso
   - PreparaciÃ³n para GitHub Actions y FTP

CARACTERÃSTICAS PRINCIPALES:
- ExtracciÃ³n robusta de todos los campos requeridos
- Manejo automÃ¡tico de cookies y sesiÃ³n
- Pausas configurables entre solicitudes
- PreservaciÃ³n de datos originales
- Logging detallado del proceso
- Compatibilidad con GitHub Actions

ARCHIVOS GENERADOS:
- 01events_YYYY-MM-DD.json          â†’ Eventos bÃ¡sicos
- 02competiciones_detalladas_YYYY-MM-DD.json â†’ Info enriquecida
- participantes_<event_id>.json     â†’ Participantes por evento
- 03todos_participantes_YYYY-MM-DD.json â†’ Todos los participantes
- participants_completos_final.json â†’ Archivo final unificado
- participantes_procesado_YYYY-MM-DD.csv â†’ CSV procesado

USO:
python extraerParticipantesEventosProx.py [--module events|info|participants|all]
"""

import os
import sys
import json
import csv
import re
import time
import argparse
import traceback
import unicodedata
import random
from datetime import datetime, timedelta
from urllib.parse import urljoin
from pathlib import Path
from glob import glob

# Third-party imports
try:
    import pandas as pd
    import numpy as np
    from dateutil import parser
    from bs4 import BeautifulSoup
    from dotenv import load_dotenv
    HAS_PANDAS = True
except ImportError as e:
    print(f"âŒ Error importando pandas/numpy: {e}")
    HAS_PANDAS = False

# Selenium imports
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import (
        TimeoutException, NoSuchElementException, WebDriverException,
        JavascriptException, StaleElementReferenceException, ElementClickInterceptedException
    )
    from selenium.webdriver.chrome.service import Service
    HAS_SELENIUM = True
except ImportError as e:
    print(f"âŒ Error importando Selenium: {e}")
    HAS_SELENIUM = False

try:
    from webdriver_manager.chrome import ChromeDriverManager
    HAS_WEBDRIVER_MANAGER = True
except ImportError as e:
    print(f"âŒ Error importando webdriver-manager: {e}")
    HAS_WEBDRIVER_MANAGER = False

# ============================== CONFIGURACIÃ“N GLOBAL ==============================

# ConfiguraciÃ³n base
BASE = "https://www.flowagility.com"
EVENTS_URL = f"{BASE}/zone/events"
SCRIPT_DIR = Path(__file__).resolve().parent

# Cargar variables de entorno
try:
    load_dotenv(SCRIPT_DIR / ".env")
    print("âœ… Variables de entorno cargadas")
except Exception as e:
    print(f"âŒ Error cargando .env: {e}")

# Credenciales
FLOW_EMAIL = os.getenv("FLOW_EMAIL", "pilar1959suarez@gmail.com")
FLOW_PASS = os.getenv("FLOW_PASS", "Seattle1")

# Flags/tunables
HEADLESS = os.getenv("HEADLESS", "true").lower() == "true"
INCOGNITO = os.getenv("INCOGNITO", "true").lower() == "true"
MAX_SCROLLS = int(os.getenv("MAX_SCROLLS", "10"))
SCROLL_WAIT_S = float(os.getenv("SCROLL_WAIT_S", "2.0"))
OUT_DIR = os.getenv("OUT_DIR", "./output")

print(f"ğŸ“‹ ConfiguraciÃ³n: HEADLESS={HEADLESS}, OUT_DIR={OUT_DIR}")

# ============================== UTILIDADES GENERALES ==============================

def log(message):
    """FunciÃ³n de logging"""
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

def slow_pause(min_s=1, max_s=2):
    """Pausa aleatoria"""
    time.sleep(random.uniform(min_s, max_s))

def _clean(s: str) -> str:
    """Limpia y normaliza texto"""
    if not s:
        return ""
    s = str(s)
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip(" \t\r\n-â€¢*Â·:;")

# ============================== FUNCIONES DE NAVEGACIÃ“N ==============================

def _get_driver(headless=True):
    """Crea y configura el driver de Selenium"""
    if not HAS_SELENIUM:
        raise ImportError("Selenium no estÃ¡ instalado")
    
    opts = Options()
    if headless:
        opts.add_argument("--headless=new")
    if INCOGNITO:
        opts.add_argument("--incognito")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1920,1080")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    try:
        if HAS_WEBDRIVER_MANAGER:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=opts)
        else:
            driver = webdriver.Chrome(options=opts)
        
        driver.set_page_load_timeout(60)
        return driver
        
    except Exception as e:
        log(f"Error creando driver: {e}")
        return None

def _login(driver):
    """Inicia sesiÃ³n en FlowAgility"""
    if not driver:
        return False
        
    log("Iniciando login...")
    
    try:
        driver.get(f"{BASE}/user/login")
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        slow_pause(2, 3)
        
        # Buscar campos de login
        email_field = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.NAME, "user[email]"))
        )
        password_field = driver.find_element(By.NAME, "user[password]")
        submit_button = driver.find_element(By.CSS_SELECTOR, 'button[type="submit"]')
        
        # Llenar campos
        email_field.clear()
        email_field.send_keys(FLOW_EMAIL)
        slow_pause(1, 2)
        
        password_field.clear()
        password_field.send_keys(FLOW_PASS)
        slow_pause(1, 2)
        
        # Hacer clic
        submit_button.click()
        
        # Esperar a que se complete el login
        WebDriverWait(driver, 30).until(
            lambda d: "/user/login" not in d.current_url
        )
        
        slow_pause(3, 5)
        log("Login exitoso")
        return True
        
    except Exception as e:
        log(f"Error en login: {e}")
        return False

def _accept_cookies(driver):
    """Aceptar cookies si es necesario"""
    try:
        cookie_selectors = [
            'button[aria-label="Accept all"]',
            'button[aria-label="Aceptar todo"]',
            '[data-testid="uc-accept-all-button"]',
            'button[mode="primary"]'
        ]
        
        for selector in cookie_selectors:
            try:
                cookie_btn = driver.find_elements(By.CSS_SELECTOR, selector)
                if cookie_btn:
                    cookie_btn[0].click()
                    slow_pause(0.5, 1)
                    log("Cookies aceptadas")
                    return True
            except:
                continue
                
        # Fallback con JavaScript
        driver.execute_script("""
            const buttons = document.querySelectorAll('button');
            for (const btn of buttons) {
                if (/aceptar|accept|consent|agree/i.test(btn.textContent)) {
                    btn.click();
                    break;
                }
            }
        """)
        slow_pause(0.5, 1)
        return True
        
    except Exception as e:
        log(f"Error manejando cookies: {e}")
        return False

def _full_scroll(driver):
    """Scroll completo para cargar todos los elementos"""
    last_height = driver.execute_script("return document.body.scrollHeight")
    for _ in range(MAX_SCROLLS):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_WAIT_S)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

# ============================== MÃ“DULO 1: EXTRACCIÃ“N DE EVENTOS ==============================

def extract_events():
    """FunciÃ³n principal para extraer eventos bÃ¡sicos - VERSIÃ“N MEJORADA"""
    if not HAS_SELENIUM:
        log("Error: Selenium no estÃ¡ instalado")
        return None
    
    log("=== MÃ“DULO 1: EXTRACCIÃ“N DE EVENTOS BÃSICOS ===")
    
    driver = _get_driver(headless=HEADLESS)
    if not driver:
        log("âŒ No se pudo crear el driver de Chrome")
        return None
    
    try:
        if not _login(driver):
            raise Exception("No se pudo iniciar sesiÃ³n")
        
        # Navegar a eventos
        log("Navegando a la pÃ¡gina de eventos...")
        driver.get(EVENTS_URL)
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # Aceptar cookies
        _accept_cookies(driver)
        
        # Scroll completo para cargar todos los eventos
        log("Cargando todos los eventos...")
        _full_scroll(driver)
        slow_pause(2, 3)
        
        # Obtener HTML de la pÃ¡gina
        page_html = driver.page_source
        
        # Guardar HTML para debugging
        debug_html_path = os.path.join(OUT_DIR, "debug_page.html")
        with open(debug_html_path, 'w', encoding='utf-8') as f:
            f.write(page_html)
        log(f"HTML de la pÃ¡gina guardado en: {debug_html_path}")
        
        # Extraer eventos usando BeautifulSoup
        log("Extrayendo informaciÃ³n de eventos...")
        soup = BeautifulSoup(page_html, 'html.parser')
        
        # Buscar contenedores de eventos
        event_containers = soup.find_all('div', class_='group mb-6')
        log(f"Encontrados {len(event_containers)} contenedores de eventos")
        
        events = []
        for i, container in enumerate(event_containers, 1):
            try:
                # Extraer informaciÃ³n completa del evento
                event_data = {}
                
                # ID del evento
                event_id = container.get('id', '')
                if event_id:
                    event_data['id'] = event_id.replace('event-card-', '')
                
                # Nombre del evento
                name_elem = container.find('div', class_='font-caption text-lg text-black truncate -mt-1')
                if name_elem:
                    event_data['nombre'] = _clean(name_elem.get_text())
                
                # Fechas
                date_elem = container.find('div', class_='text-xs')
                if date_elem:
                    event_data['fechas'] = _clean(date_elem.get_text())
                
                # OrganizaciÃ³n
                org_elems = container.find_all('div', class_='text-xs')
                if len(org_elems) > 1:
                    event_data['organizacion'] = _clean(org_elems[1].get_text())
                
                # Club organizador - BUSCAR ESPECÃFICAMENTE
                club_elem = container.find('div', class_='text-xs mb-0.5 mt-0.5')
                if club_elem:
                    event_data['club'] = _clean(club_elem.get_text())
                else:
                    # Fallback: buscar en todos los divs con text-xs
                    for div in container.find_all('div', class_='text-xs'):
                        text = _clean(div.get_text())
                        if text and not any(x in text for x in ['/', 'Spain', 'EspaÃ±a']):
                            event_data['club'] = text
                            break
                
                # Lugar - BUSCAR PATRÃ“N CIUDAD/PAÃS
                location_divs = container.find_all('div', class_='text-xs')
                for div in location_divs:
                    text = _clean(div.get_text())
                    if '/' in text and any(x in text for x in ['Spain', 'EspaÃ±a', 'Madrid', 'Barcelona']):
                        event_data['lugar'] = text
                        break
                
                # Si no encontramos lugar, buscar cualquier texto con /
                if 'lugar' not in event_data:
                    for div in location_divs:
                        text = _clean(div.get_text())
                        if '/' in text and len(text) < 100:  # Evitar textos muy largos
                            event_data['lugar'] = text
                            break
                
                # Enlaces
                event_data['enlaces'] = {}
                
                # Enlace de informaciÃ³n
                info_link = container.find('a', href=lambda x: x and '/info/' in x)
                if info_link:
                    event_data['enlaces']['info'] = urljoin(BASE, info_link['href'])
                
                # Enlace de participantes - BUSCAR EXPLÃCITAMENTE
                participant_links = container.find_all('a', href=lambda x: x and any(term in x for term in ['/participants', '/participantes']))
                for link in participant_links:
                    href = link.get('href', '')
                    if '/participants_list' in href or '/participantes' in href:
                        event_data['enlaces']['participantes'] = urljoin(BASE, href)
                        break
                
                # Si no encontramos el enlace de participantes, construirlo
                if 'participantes' not in event_data['enlaces'] and 'id' in event_data:
                    event_data['enlaces']['participantes'] = f"{BASE}/zone/events/{event_data['id']}/participants_list"
                
                # Bandera del paÃ­s
                flag_elem = container.find('div', class_='text-md')
                if flag_elem:
                    event_data['pais_bandera'] = _clean(flag_elem.get_text())
                else:
                    event_data['pais_bandera'] = 'ğŸ‡ªğŸ‡¸'  # Valor por defecto
                
                events.append(event_data)
                log(f"âœ… Evento {i} procesado: {event_data.get('nombre', 'Sin nombre')}")
                
            except Exception as e:
                log(f"âŒ Error procesando evento {i}: {str(e)}")
                continue
        
        # Guardar resultados
        today_str = datetime.now().strftime("%Y-%m-%d")
        output_file = os.path.join(OUT_DIR, f'01events_{today_str}.json')
        os.makedirs(OUT_DIR, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(events, f, ensure_ascii=False, indent=2)
        
        log(f"âœ… ExtracciÃ³n completada. {len(events)} eventos guardados en {output_file}")
        
        # Mostrar resumen de lo extraÃ­do
        print(f"\n{'='*80}")
        print("RESUMEN DE CAMPOS EXTRAÃDOS:")
        print(f"{'='*80}")
        for event in events[:5]:  # Mostrar primeros 5 eventos como ejemplo
            print(f"\nEvento: {event.get('nombre', 'N/A')}")
            print(f"  Club: {event.get('club', 'No extraÃ­do')}")
            print(f"  Lugar: {event.get('lugar', 'No extraÃ­do')}")
            print(f"  Enlace participantes: {event.get('enlaces', {}).get('participantes', 'No extraÃ­do')}")
        print(f"\n{'='*80}")
        
        return events
        
    except Exception as e:
        log(f"âŒ Error durante el scraping: {str(e)}")
        traceback.print_exc()
        return None
    finally:
        try:
            driver.quit()
            log("Navegador cerrado")
        except:
            pass

# ============================== MÃ“DULO 2: INFORMACIÃ“N DETALLADA ==============================

def extract_detailed_info():
    """Extraer informaciÃ³n detallada de cada evento"""
    if not HAS_SELENIUM:
        log("Error: Selenium no estÃ¡ instalado")
        return None
    
    log("=== MÃ“DULO 2: EXTRACCIÃ“N DE INFORMACIÃ“N DETALLADA ===")
    
    # Buscar el archivo de eventos mÃ¡s reciente
    event_files = glob(os.path.join(OUT_DIR, "01events_*.json"))
    if not event_files:
        log("âŒ No se encontraron archivos de eventos")
        return None
    
    latest_event_file = max(event_files, key=os.path.getctime)
    
    # Cargar eventos
    with open(latest_event_file, 'r', encoding='utf-8') as f:
        events = json.load(f)
    
    log(f"âœ… Cargados {len(events)} eventos desde {latest_event_file}")
    
    driver = _get_driver(headless=HEADLESS)
    if not driver:
        log("âŒ No se pudo crear el driver de Chrome")
        return None
    
    try:
        if not _login(driver):
            raise Exception("No se pudo iniciar sesiÃ³n")
        
        detailed_events = []
        
        for i, event in enumerate(events, 1):
            try:
                # PRESERVAR CAMPOS ORIGINALES IMPORTANTES
                preserved_fields = ['id', 'nombre', 'fechas', 'organizacion', 'club', 'lugar', 'enlaces', 'pais_bandera']
                detailed_event = {field: event.get(field, '') for field in preserved_fields}
                
                # Verificar si tiene enlace de informaciÃ³n
                if 'enlaces' in event and 'info' in event['enlaces']:
                    info_url = event['enlaces']['info']
                    
                    log(f"Procesando evento {i}/{len(events)}: {event.get('nombre', 'Sin nombre')}")
                    
                    # Navegar a la pÃ¡gina de informaciÃ³n
                    driver.get(info_url)
                    WebDriverWait(driver, 20).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    
                    slow_pause(2, 3)
                    
                    # Obtener HTML de la pÃ¡gina
                    page_html = driver.page_source
                    soup = BeautifulSoup(page_html, 'html.parser')
                    
                    # ===== INFORMACIÃ“N ADICIONAL =====
                    additional_info = {}
                    
                    # Intentar mejorar informaciÃ³n de club si no estÃ¡ completa
                    if not detailed_event.get('club') or detailed_event.get('club') in ['N/D', '']:
                        club_elems = soup.find_all(lambda tag: any(word in tag.get_text().lower() for word in ['club', 'organizador', 'organizer']))
                        for elem in club_elems:
                            text = _clean(elem.get_text())
                            if text and len(text) < 100:  # Evitar textos muy largos
                                detailed_event['club'] = text
                                break
                    
                    # Intentar mejorar informaciÃ³n de lugar si no estÃ¡ completa
                    if not detailed_event.get('lugar') or detailed_event.get('lugar') in ['N/D', '']:
                        location_elems = soup.find_all(lambda tag: any(word in tag.get_text().lower() for word in ['lugar', 'ubicacion', 'location', 'place']))
                        for elem in location_elems:
                            text = _clean(elem.get_text())
                            if text and ('/' in text or any(x in text for x in ['Spain', 'EspaÃ±a'])):
                                detailed_event['lugar'] = text
                                break
                    
                    # Extraer informaciÃ³n general adicional
                    title_elem = soup.find('h1')
                    if title_elem:
                        additional_info['titulo_completo'] = _clean(title_elem.get_text())
                    
                    # Extraer descripciÃ³n si existe
                    description_elem = soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['description', 'descripcion', 'info']))
                    if description_elem:
                        additional_info['descripcion'] = _clean(description_elem.get_text())
                    
                    # AÃ±adir informaciÃ³n adicional al evento
                    detailed_event['informacion_adicional'] = additional_info
                    detailed_event['timestamp_extraccion'] = datetime.now().isoformat()
                    
                else:
                    log(f"Evento {i} no tiene enlace de informaciÃ³n, usando datos bÃ¡sicos")
                
                detailed_events.append(detailed_event)
                slow_pause(1, 2)  # Pausa entre solicitudes
                
            except Exception as e:
                log(f"âŒ Error procesando evento {i}: {str(e)}")
                detailed_events.append(event)  # Mantener datos originales
                continue
        
        # Guardar informaciÃ³n detallada
        today_str = datetime.now().strftime("%Y-%m-%d")
        output_file = os.path.join(OUT_DIR, f'02competiciones_detalladas_{today_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_events, f, ensure_ascii=False, indent=2)
        
        log(f"âœ… InformaciÃ³n detallada guardada en {output_file}")
        return detailed_events
        
    except Exception as e:
        log(f"âŒ Error durante la extracciÃ³n detallada: {str(e)}")
        traceback.print_exc()
        return None
    finally:
        try:
            driver.quit()
        except:
            pass
# ============================== MÃ“DULO 4: GENERACIÃ“N DE ARCHIVO FINAL ==============================

def generate_final_json():
    """Genera el archivo final JSON que espera GitHub Actions"""
    log("Generando archivo final de unificaciÃ³n...")
    
    # Buscar archivo mÃ¡s reciente de participantes
    participant_files = glob(os.path.join(OUT_DIR, "participantes_procesado_*.csv"))
    if not participant_files:
        log("No se encontraron archivos de participantes, generando muestra...")
        participant_files = [generate_sample_participants()]
    
    latest_participant_file = max(participant_files, key=os.path.getctime)
    
    try:
        # Leer CSV y convertir a JSON
        df = pd.read_csv(latest_participant_file)
        final_data = df.to_dict('records')
        
        # Guardar como JSON
        final_file = os.path.join(OUT_DIR, "participants_completos_final.json")
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        
        log(f"âœ… Archivo final generado: {final_file}")
        return True
        
    except Exception as e:
        log(f"âŒ Error generando archivo final: {e}")
        return False
       
# ============================== MÃ“DULO 3: EXTRACCIÃ“N DE PARTICIPANTES ==============================

def extract_participants():
    """Extraer informaciÃ³n detallada de participantes usando tÃ©cnicas del script original"""
    if not HAS_SELENIUM:
        log("Error: Selenium no estÃ¡ instalado")
        return None
    
    log("=== MÃ“DULO 3: EXTRACCIÃ“N DE PARTICIPANTES ===")
    
    # Importar componentes de Selenium necesarios
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    
    # Buscar el archivo de eventos detallados mÃ¡s reciente
    detailed_files = glob(os.path.join(OUT_DIR, "02competiciones_detalladas_*.json"))
    if not detailed_files:
        log("âŒ No se encontraron archivos de eventos detallados")
        return None
    
    latest_detailed_file = max(detailed_files, key=os.path.getctime)
    
    # Cargar eventos detallados
    with open(latest_detailed_file, 'r', encoding='utf-8') as f:
        events = json.load(f)
    
    log(f"âœ… Cargados {len(events)} eventos detallados desde {latest_detailed_file}")
    
    driver = _get_driver(headless=False)  # headless=False para ver el proceso
    if not driver:
        log("âŒ No se pudo crear el driver de Chrome")
        return None
    
    try:
        if not _login(driver):
            raise Exception("No se pudo iniciar sesiÃ³n")
        
        all_participants = []
        events_with_participants = 0
        
        for i, event in enumerate(events, 1):
            try:
                # Verificar si tiene enlace de participantes
                if 'enlaces' in event and 'participantes' in event['enlaces']:
                    participants_url = event['enlaces']['participantes']
                    
                    log(f"ğŸ“‹ Procesando participantes {i}/{len(events)}: {event.get('nombre', 'Sin nombre')}")
                    log(f"   URL: {participants_url}")
                    
                    # Navegar a la pÃ¡gina de participantes
                    driver.get(participants_url)
                    WebDriverWait(driver, 20).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    
                    slow_pause(3, 5)
                    
                    # Extraer informaciÃ³n detallada de participantes
                    participants_data = _extract_detailed_participants_selenium(driver, participants_url, event)
                    
                    if participants_data:
                        events_with_participants += 1
                        log(f"  âœ… Encontrados {len(participants_data)} participantes")
                        
                        # Guardar participantes de este evento
                        event_participants_file = os.path.join(OUT_DIR, f"participantes_{event.get('id', 'unknown')}.json")
                        with open(event_participants_file, 'w', encoding='utf-8') as f:
                            json.dump(participants_data, f, ensure_ascii=False, indent=2)
                        
                        all_participants.extend(participants_data)
                    else:
                        log(f"  âš ï¸  No se encontraron participantes")
                
                else:
                    log(f"Evento {i} no tiene enlace de participantes")
                
                slow_pause(2, 3)  # Pausa entre eventos
                
            except Exception as e:
                log(f"âŒ Error procesando participantes del evento {i}: {str(e)}")
                continue
        
        # Guardar todos los participantes
        if all_participants:
            today_str = datetime.now().strftime("%Y-%m-%d")
            output_file = os.path.join(OUT_DIR, f'03todos_participantes_{today_str}.json')
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_participants, f, ensure_ascii=False, indent=2)
            
            log(f"âœ… Total de {len(all_participants)} participantes guardados en {output_file}")
            log(f"âœ… {events_with_participants} eventos con participantes procesados")
            
            # Generar CSV inmediatamente despuÃ©s
            generate_csv_output()
        else:
            log("âš ï¸  No se encontraron participantes en ningÃºn evento")
        
        return all_participants
        
    except Exception as e:
        log(f"âŒ Error durante la extracciÃ³n de participantes: {str(e)}")
        traceback.print_exc()
        return None
    finally:
        try:
            driver.quit()
        except:
            pass

def _extract_detailed_participants_selenium(driver, participants_url, event):
    """Extraer informaciÃ³n detallada de participantes usando Selenium como en el script original"""
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    
    participants = []
    
    try:
        # Esperar a que cargue la pÃ¡gina de participantes
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # Buscar botones de "Ver detalles" o elementos clickables
        detail_buttons = driver.find_elements(By.CSS_SELECTOR, "[phx-click^='booking_details_'], [onclick*='details'], .btn-details, button")
        
        log(f"  Encontrados {len(detail_buttons)} botones/interacciones posibles")
        
        # Si no encontramos botones especÃ­ficos, buscar tablas o listas de participantes
        if not detail_buttons:
            log("  âš ï¸  No se encontraron botones de detalles, buscando informaciÃ³n directa...")
            participants = _extract_participants_from_direct_info(driver, participants_url, event)
        else:
            # Intentar hacer clic en cada botÃ³n para extraer informaciÃ³n
            for i, button in enumerate(detail_buttons[:10]):  # Limitar para prueba
                try:
                    participant_data = _extract_participant_from_button(driver, button, participants_url, event)
                    if participant_data:
                        participants.append(participant_data)
                        log(f"    âœ… Participante {len(participants)} extraÃ­do")
                except Exception as e:
                    log(f"    âŒ Error en botÃ³n {i+1}: {e}")
                    continue
        
        return participants
        
    except Exception as e:
        log(f"Error en extracciÃ³n Selenium: {e}")
        return []

def _extract_participant_from_button(driver, button, participants_url, event):
    """Extraer informaciÃ³n de un participante haciendo clic en su botÃ³n"""
    try:
        # Hacer clic en el botÃ³n
        driver.execute_script("arguments[0].click();", button)
        slow_pause(1, 2)
        
        # Buscar el modal o panel de detalles que se abre
        modal = driver.find_elements(By.CSS_SELECTOR, ".modal-content, .details-panel, [role='dialog']")
        
        if modal:
            # Extraer informaciÃ³n del modal
            participant_data = _extract_from_modal(modal[0], participants_url, event)
            # Cerrar el modal
            close_buttons = driver.find_elements(By.CSS_SELECTOR, ".close, [aria-label='Close'], .btn-close")
            if close_buttons:
                driver.execute_script("arguments[0].click();", close_buttons[0])
            
            return participant_data
        
        return None
        
    except Exception as e:
        log(f"Error extrayendo de botÃ³n: {e}")
        return None

def _extract_from_modal(modal_element, participants_url, event):
    """Extraer informaciÃ³n del modal de detalles"""
    participant_data = {
        'participants_url': participants_url,
        'BinomID': '',
        'Dorsal': '',
        'GuÃ­a': '',
        'Perro': '',
        'Raza': '',
        'Edad': '',
        'GÃ©nero': '',
        'Altura (cm)': '',
        'Nombre de Pedigree': '',
        'PaÃ­s': 'No disponible',
        'Licencia': '',
        'Club': '',
        'FederaciÃ³n': '',
        'Equipo': 'No disponible',
        'event_uuid': event.get('id', ''),
        'event_title': event.get('nombre', 'N/D')
    }
    
    try:
        # Obtener todo el texto del modal
        modal_text = modal_element.text
        
        # Extraer informaciÃ³n usando patrones (similar al script original)
        lines = [line.strip() for line in modal_text.split('\n') if line.strip()]
        
        for line in lines:
            # Dorsal
            if not participant_data['Dorsal'] and re.match(r'^\d+$', line):
                participant_data['Dorsal'] = line
            
            # GuÃ­a (nombres con apellidos)
            if not participant_data['GuÃ­a'] and re.match(r'^[A-ZÃÃ‰ÃÃ“ÃšÃœ][a-zÃ¡Ã©Ã­Ã³ÃºÃ¼]+\s+[A-ZÃÃ‰ÃÃ“ÃšÃœ][a-zÃ¡Ã©Ã­Ã³ÃºÃ¼]+$', line):
                participant_data['GuÃ­a'] = line
            
            # Perro (palabras en mayÃºsculas o mixed case)
            if not participant_data['Perro'] and re.match(r'^[A-ZÃÃ‰ÃÃ“ÃšÃœ][a-zA-ZÃÃ‰ÃÃ“ÃšÃœÃ¡Ã©Ã­Ã³ÃºÃ¼]+$', line):
                participant_data['Perro'] = line
            
            # InformaciÃ³n especÃ­fica con labels
            if ':' in line:
                parts = line.split(':', 1)
                key = parts[0].strip().lower()
                value = parts[1].strip()
                
                if 'guÃ­a' in key or 'handler' in key:
                    participant_data['GuÃ­a'] = value
                elif 'perro' in key or 'dog' in key:
                    participant_data['Perro'] = value
                elif 'raza' in key or 'breed' in key:
                    participant_data['Raza'] = value
                elif 'edad' in key or 'age' in key:
                    participant_data['Edad'] = value
                elif 'gÃ©nero' in key or 'gender' in key or 'sex' in key:
                    participant_data['GÃ©nero'] = value
                elif 'altura' in key or 'height' in key:
                    participant_data['Altura (cm)'] = value
                elif 'licencia' in key or 'license' in key:
                    participant_data['Licencia'] = value
                elif 'club' in key:
                    participant_data['Club'] = value
                elif 'federaciÃ³n' in key or 'federation' in key:
                    participant_data['FederaciÃ³n'] = value
        
        # Generar BinomID
        if participant_data['GuÃ­a'] and participant_data['Perro']:
            participant_data['BinomID'] = f"{participant_data['GuÃ­a']}_{participant_data['Perro']}".replace(' ', '_').lower()
        
        return participant_data
        
    except Exception as e:
        log(f"Error extrayendo del modal: {e}")
        return participant_data

def _extract_participants_from_direct_info(driver, participants_url, event):
    """Extraer informaciÃ³n de participantes directamente de la pÃ¡gina (fallback)"""
    participants = []
    
    try:
        # Obtener todo el texto de la pÃ¡gina
        page_text = driver.find_element(By.TAG_NAME, "body").text
        lines = [line.strip() for line in page_text.split('\n') if line.strip()]
        
        # Buscar secciones que parezcan contener informaciÃ³n de participantes
        participant_lines = []
        current_section = []
        
        for line in lines:
            if re.match(r'^\d+\.', line) or re.match(r'^\d+\s', line) or any(keyword in line.lower() for keyword in ['dorsal', 'guÃ­a', 'perro', 'handler']):
                if current_section:
                    participant_lines.append(current_section)
                current_section = [line]
            elif current_section:
                current_section.append(line)
        
        if current_section:
            participant_lines.append(current_section)
        
        # Procesar cada secciÃ³n de participante
        for section in participant_lines:
            participant_data = _parse_participant_section(section, participants_url, event)
            if participant_data:
                participants.append(participant_data)
        
        return participants
        
    except Exception as e:
        log(f"Error en extracciÃ³n directa: {e}")
        return []

def _parse_participant_section(section_lines, participants_url, event):
    """Parsear una secciÃ³n de texto para extraer informaciÃ³n de participante"""
    participant_data = {
        'participants_url': participants_url,
        'BinomID': '',
        'Dorsal': '',
        'GuÃ­a': '',
        'Perro': '',
        'Raza': '',
        'Edad': '',
        'GÃ©nero': '',
        'Altura (cm)': '',
        'Nombre de Pedigree': '',
        'PaÃ­s': 'No disponible',
        'Licencia': '',
        'Club': '',
        'FederaciÃ³n': '',
        'Equipo': 'No disponible',
        'event_uuid': event.get('id', ''),
        'event_title': event.get('nombre', 'N/D')
    }
    
    try:
        section_text = ' '.join(section_lines)
        
        # Buscar dorsal (nÃºmero al inicio)
        dorsal_match = re.search(r'^(\d+)', section_lines[0] if section_lines else '')
        if dorsal_match:
            participant_data['Dorsal'] = dorsal_match.group(1)
        
        # Buscar nombre de guÃ­a (patrÃ³n nombre + apellido)
        for line in section_lines:
            if re.match(r'^[A-ZÃÃ‰ÃÃ“ÃšÃœ][a-zÃ¡Ã©Ã­Ã³ÃºÃ¼]+\s+[A-ZÃÃ‰ÃÃ“ÃšÃœ][a-zÃ¡Ã©Ã­Ã³ÃºÃ¼]+$', line):
                participant_data['GuÃ­a'] = line
                break
        
        # Buscar nombre de perro (palabra en mayÃºsculas o tÃ­tulo)
        for line in section_lines:
            if re.match(r'^[A-ZÃÃ‰ÃÃ“ÃšÃœ][a-zA-ZÃÃ‰ÃÃ“ÃšÃœÃ¡Ã©Ã­Ã³ÃºÃ¼]{3,}$', line) and line != participant_data['GuÃ­a']:
                participant_data['Perro'] = line
                break
        
        # Generar BinomID
        if participant_data['GuÃ­a'] and participant_data['Perro']:
            participant_data['BinomID'] = f"{participant_data['GuÃ­a']}_{participant_data['Perro']}".replace(' ', '_').lower()
        
        return participant_data
        
    except Exception as e:
        log(f"Error parseando secciÃ³n: {e}")
        return None

# ============================== MÃ“DULO 4: GENERACIÃ“N DE ARCHIVOS FINALES ==============================

def generate_csv_output():
    """Generar archivo CSV procesado con la estructura requerida"""
    log("=== GENERANDO ARCHIVO CSV PROCESADO ===")
    
    # Buscar archivo de participantes mÃ¡s reciente
    participant_files = glob(os.path.join(OUT_DIR, "03todos_participantes_*.json"))
    if not participant_files:
        log("âŒ No se encontraron archivos de participantes")
        return False
    
    latest_participant_file = max(participant_files, key=os.path.getctime)
    
    # Cargar participantes
    with open(latest_participant_file, 'r', encoding='utf-8') as f:
        participants = json.load(f)
    
    if not participants:
        log("âš ï¸  No hay participantes para procesar")
        return False
    
    # Definir campos para el CSV (exactamente como los necesitas)
    fieldnames = [
        'participants_url', 'BinomID', 'Dorsal', 'GuÃ­a', 'Perro', 'Raza', 'Edad', 
        'GÃ©nero', 'Altura (cm)', 'Nombre de Pedigree', 'PaÃ­s', 'Licencia', 'Club', 
        'FederaciÃ³n', 'Equipo', 'event_uuid', 'event_title'
    ]
    
    # AÃ±adir campos de dÃ­as/mangas (DÃ­a 1-6, Fecha 1-6, Mangas 1-6)
    for i in range(1, 7):
        fieldnames.extend([f'DÃ­a {i}', f'Fecha {i}', f'Mangas {i}'])
    
    # Guardar como CSV
    today_str = datetime.now().strftime("%Y-%m-%d")
    csv_file = os.path.join(OUT_DIR, f'participantes_procesado_{today_str}.csv')
    
    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for participant in participants:
            # Asegurar que todos los campos existan y reemplazar NaN/None por ''
            row = {}
            for field in fieldnames:
                value = participant.get(field, '')
                if value is None or (isinstance(value, float) and np.isnan(value)):
                    value = ''
                row[field] = value
            writer.writerow(row)
    
    log(f"âœ… Archivo CSV generado: {csv_file}")
    
    # Mostrar ejemplo del primer participante
    if participants:
        first_participant = participants[0]
        print(f"\nğŸ“‹ EJEMPLO DE PARTICIPANTE EXTRAÃDO:")
        for field in ['GuÃ­a', 'Perro', 'Dorsal', 'Club', 'Raza']:
            value = first_participant.get(field, 'N/A')
            if value:
                print(f"   {field}: {value}")
    
    return True
def generate_final_json():
    """Generar el archivo JSON final unificado"""
    log("=== GENERANDO ARCHIVO JSON FINAL ===")
    
    # Buscar archivos mÃ¡s recientes
    event_files = glob(os.path.join(OUT_DIR, "01events_*.json"))
    detailed_files = glob(os.path.join(OUT_DIR, "02competiciones_detalladas_*.json"))
    participant_files = glob(os.path.join(OUT_DIR, "03todos_participantes_*.json"))
    
    if not event_files:
        log("âŒ No se encontraron archivos de eventos")
        return False
    
    # Cargar eventos
    latest_event_file = max(event_files, key=os.path.getctime)
    with open(latest_event_file, 'r', encoding='utf-8') as f:
        events = json.load(f)
    
    # Cargar informaciÃ³n detallada si existe
    detailed_events = []
    if detailed_files:
        latest_detailed_file = max(detailed_files, key=os.path.getctime)
        with open(latest_detailed_file, 'r', encoding='utf-8') as f:
            detailed_events = json.load(f)
    
    # Cargar participantes si existen
    all_participants = []
    if participant_files:
        latest_participant_file = max(participant_files, key=os.path.getctime)
        with open(latest_participant_file, 'r', encoding='utf-8') as f:
            all_participants = json.load(f)
    
    # Crear estructura final
    final_data = {
        'metadata': {
            'fecha_generacion': datetime.now().isoformat(),
            'total_eventos': len(events),
            'total_eventos_detallados': len(detailed_events),
            'total_participantes': len(all_participants),
            'version': '1.0'
        },
        'eventos': events,
        'eventos_detallados': detailed_events,
        'participantes': all_participants
    }
    
    # Guardar archivo final
    final_file = os.path.join(OUT_DIR, "participants_completos_final.json")
    with open(final_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    log(f"âœ… Archivo final JSON generado: {final_file}")
    
    # Resumen final
    print(f"\n{'='*80}")
    print("RESUMEN FINAL DEL PROCESO:")
    print(f"{'='*80}")
    print(f"ğŸ“Š Eventos bÃ¡sicos: {len(events)}")
    print(f"ğŸ“Š Eventos con info detallada: {len(detailed_events)}")
    print(f"ğŸ“Š Total participantes: {len(all_participants)}")
    
    # Verificar archivos generados
    print(f"\nğŸ“ ARCHIVOS GENERADOS:")
    output_files = glob(os.path.join(OUT_DIR, "*"))
    for file in sorted(output_files):
        size = os.path.getsize(file)
        print(f"   {os.path.basename(file)} - {size} bytes")
    
    print(f"\n{'='*80}")
    
    return True
# ============================== FUNCIÃ“N PRINCIPAL ==============================

def main():
    """FunciÃ³n principal"""
    print("ğŸš€ INICIANDO FLOWAGILITY SCRAPER COMPLETO")
    print("ğŸ“‹ Este proceso realizarÃ¡ la extracciÃ³n completa de datos de competiciones")
    print(f"ğŸ“‚ Directorio de salida: {OUT_DIR}")
    print("=" * 80)
    
    # Crear directorio de salida
    os.makedirs(OUT_DIR, exist_ok=True)
    
    parser = argparse.ArgumentParser(description="FlowAgility Scraper Mejorado")
    parser.add_argument("--module", choices=["events", "info", "participants", "csv", "all"], default="all", help="MÃ³dulo a ejecutar")
    args = parser.parse_args()
    
    try:
        success = True
        
        # MÃ³dulo 1: Eventos bÃ¡sicos
        if args.module in ["events", "all"]:
            log("ğŸ INICIANDO EXTRACCIÃ“N DE EVENTOS BÃSICOS")
            events = extract_events()
            if not events:
                log("âŒ FallÃ³ la extracciÃ³n de eventos")
                success = False
            else:
                log("âœ… Eventos bÃ¡sicos extraÃ­dos correctamente")
        
        # MÃ³dulo 2: InformaciÃ³n detallada
        if args.module in ["info", "all"] and success:
            log("ğŸ INICIANDO EXTRACCIÃ“N DE INFORMACIÃ“N DETALLADA")
            detailed_events = extract_detailed_info()
            if not detailed_events:
                log("âš ï¸  No se pudo extraer informaciÃ³n detallada, continuando con datos bÃ¡sicos")
            else:
                log("âœ… InformaciÃ³n detallada extraÃ­da correctamente")
        
        # MÃ³dulo 3: Participantes
        if args.module in ["participants", "all"] and success:
            log("ğŸ INICIANDO EXTRACCIÃ“N DE PARTICIPANTES")
            participants = extract_participants()
            if not participants:
                log("âš ï¸  No se pudo extraer participantes, continuando sin ellos")
            else:
                log("âœ… Participantes extraÃ­dos correctamente")
        
        # MÃ³dulo 4: CSV Procesado
        if args.module in ["csv", "all"] and success:
            log("ğŸ GENERANDO ARCHIVO CSV PROCESADO")
            if not generate_csv_output():
                log("âš ï¸  No se pudo generar el archivo CSV")
            else:
                log("âœ… Archivo CSV generado correctamente")
        
        # Archivo final JSON
        if args.module in ["all"] and success:
            log("ğŸ GENERANDO ARCHIVO FINAL JSON")
            if not generate_final_json():
                log("âŒ FallÃ³ la generaciÃ³n del archivo final JSON")
                success = False
            else:
                log("âœ… Archivo final JSON generado correctamente")
        
        if success:
            log("ğŸ‰ PROCESO COMPLETADO EXITOSAMENTE")
            print("\nâœ… Todos los mÃ³dulos se ejecutaron correctamente")
            print("ğŸ“Š Los archivos estÃ¡n listos para GitHub Actions y FTP")
        else:
            log("âŒ PROCESO COMPLETADO CON ERRORES")
            print("\nâš ï¸  Algunos mÃ³dulos tuvieron errores")
            print("ğŸ“‹ Revisa los logs para mÃ¡s detalles")
        
        return success
        
    except Exception as e:
        log(f"âŒ ERROR CRÃTICO DURANTE LA EJECUCIÃ“N: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
